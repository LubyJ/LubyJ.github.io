{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb160e47",
   "metadata": {},
   "source": [
    "영어로 만들었던 챗봇을 한국어 데이터로 바꿔서 훈련시켜봅시다.\n",
    "\n",
    "시작하기 전에 우선 주요 라이브러리 버전을 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a72bd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7abb71",
   "metadata": {},
   "source": [
    "## Step 1. 데이터 수집하기\n",
    "---\n",
    "한국어 챗봇 데이터는 송영숙님이 공개한 챗봇 데이터를 사용합니다.\n",
    "\n",
    "이 데이터는 아래의 링크에서 다운로드할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc5273ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '../transformer_chatbot/data/ChatbotData .csv'\n",
    "data = pd.read_csv(file_path, header=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9de2fbd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11823, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcb6f721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Q        0\n",
       "A        0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702c0717",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 전처리하기\n",
    "---\n",
    "영어 데이터와는 전혀 다른 데이터인 만큼 영어 데이터에 사용했던 전처리와 일부 동일한 전처리도 필요하겠지만 전체적으로는 다른 전처리를 수행해야 할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5127ad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = []\n",
    "for sentence in data.Q:\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    Q.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cceaa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = []\n",
    "for sentence in data.A:\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    A.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94d4898e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12시 땡 !', '1지망 학교 떨어졌어', '3박4일 놀러가고 싶다', '3박4일 정도 놀러가고 싶다', 'PPL 심하네']\n",
      "['하루가 또 가네요 .', '위로해 드립니다 .', '여행은 언제나 좋죠 .', '여행은 언제나 좋죠 .', '눈살이 찌푸려지죠 .']\n"
     ]
    }
   ],
   "source": [
    "print(Q[:5])\n",
    "print(A[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a644b2d",
   "metadata": {},
   "source": [
    "## Step 3. SubwordTextEncoder 사용하기\n",
    "---\n",
    "한국어 데이터는 형태소 분석기를 사용하여 토크나이징을 해야 한다고 많은 분이 알고 있습니다. 하지만 여기서는 형태소 분석기가 아닌 위 실습에서 사용했던 내부 단어 토크나이저인 SubwordTextEncoder를 그대로 사용해보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0146a68f",
   "metadata": {},
   "source": [
    "### 3.1 단어장 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dda8a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(Q + A, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099b2417",
   "metadata": {},
   "source": [
    "시작 토큰과 종료 토큰에 고유한 정수 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a34a181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_TOKEN의 번호 : [8178]\n",
      "END_TOKEN의 번호 : [8179]\n"
     ]
    }
   ],
   "source": [
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946d9345",
   "metadata": {},
   "source": [
    "시작 토큰과 종료 토큰을 고려하여 +2를 하여 단어장의 크기를 산정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65f6b4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8180\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faee0b6",
   "metadata": {},
   "source": [
    "### 3.2 각 단어를 고유 정수로 인코딩 & 패딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2e57c0",
   "metadata": {},
   "source": [
    "각 질문과 답변 샘플 한개를 인코딩, 디코딩 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f8e6163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 21번째 질문 샘플: [5766, 611, 2495, 4167]\n",
      "정수 인코딩 후의 21번째 답변 샘플: [2359, 7516, 7, 6279, 97, 1]\n"
     ]
    }
   ],
   "source": [
    "# 임의의 22번째 샘플에 대해서 정수 인코딩 작업을 수행.\n",
    "# 각 토큰을 고유한 정수로 변환\n",
    "tok_question = tokenizer.encode(Q[21])\n",
    "tok_answer = tokenizer.encode(A[21])\n",
    "print('정수 인코딩 후의 21번째 질문 샘플: {}'.format(tokenizer.encode(Q[21])))\n",
    "print('정수 인코딩 후의 21번째 답변 샘플: {}'.format(tokenizer.encode(A[21])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1645284",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "디코딩 후의 21번째 질문 샘플: 가스비 장난 아님\n",
      "디코딩 후의 21번째 답변 샘플: 다음 달에는 더 절약해봐요 .\n"
     ]
    }
   ],
   "source": [
    "print('디코딩 후의 21번째 질문 샘플: {}'.format(tokenizer.decode(tok_question)))\n",
    "print('디코딩 후의 21번째 답변 샘플: {}'.format(tokenizer.decode(tok_answer)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e238b3f",
   "metadata": {},
   "source": [
    "인코딩한 질문 샘플은 정수가 4개인데 원문의 어절은 3개임. 같은 맥락으로 답변 샘플도 인코딩된 문장은 정수가 6개인 반면, 원문의 어절은 마침표를 포함해서 5개임. 각 정수가 어떤 서브워드를 mapping하고 있는지 출력해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbb18186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5766 ----> 가스\n",
      "611 ----> 비 \n",
      "2495 ----> 장난 \n",
      "4167 ----> 아님\n"
     ]
    }
   ],
   "source": [
    "for ts in tok_question:\n",
    "    print('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6d5286f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2359 ----> 다음 \n",
      "7516 ----> 달에는 \n",
      "7 ----> 더 \n",
      "6279 ----> 절약해\n",
      "97 ----> 봐요\n",
      "1 ---->  .\n"
     ]
    }
   ],
   "source": [
    "for ts in tok_answer:\n",
    "    print('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffdc802",
   "metadata": {},
   "source": [
    "가스비가 [가스, 비]로, 절약해봐요는 [절약해, 봐요]로 나뉜상태로 정수 인코딩이 되어있음을 알 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7dc6036c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 최소 길이 : 1\n",
      "질문 최대 길이 : 16\n",
      "질문 평균 길이 : 3.9402858834475176\n",
      "답변 최소 길이 : 1\n",
      "답변 최대 길이 : 24\n",
      "답변 평균 길이 : 4.71589275141673\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "Q_len = [len(s.split()) for s in Q]\n",
    "A_len = [len(s.split()) for s in A]\n",
    "\n",
    "print('질문 최소 길이 : {}'.format(np.min(Q_len)))\n",
    "print('질문 최대 길이 : {}'.format(np.max(Q_len)))\n",
    "print('질문 평균 길이 : {}'.format(np.mean(Q_len)))\n",
    "print('답변 최소 길이 : {}'.format(np.min(A_len)))\n",
    "print('답변 최대 길이 : {}'.format(np.max(A_len)))\n",
    "print('답변 평균 길이 : {}'.format(np.mean(A_len)))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(Q_len)\n",
    "plt.title('Question')\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(A_len)\n",
    "plt.title('Answer')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Question')\n",
    "plt.hist(Q_len, bins = 30)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "\n",
    "plt.title('Answer')\n",
    "plt.hist(A_len, bins = 30)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e2c069",
   "metadata": {},
   "source": [
    "질문과 답변의 최대 길이는 16, 24로 길지 않음. 질문과 답변의 최대 길이는 SOS와 EOS를 고려하여 26을 기준으로 패딩 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4e78bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 26\n",
    "\n",
    "def tokenize_and_padding(inputs, outputs):\n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "    \n",
    "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "        # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
    "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "        \n",
    "        tokenized_inputs.append(sentence1)\n",
    "        tokenized_outputs.append(sentence2)\n",
    "        \n",
    "    # 최대 길이 26로 모든 데이터셋을 패딩\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "        \n",
    "    return tokenized_inputs, tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0195caca",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, answers = tokenize_and_padding(Q, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb39b5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 데이터의 크기(shape) : (11823, 26)\n",
      "답변 데이터의 크기(shape) : (11823, 26)\n"
     ]
    }
   ],
   "source": [
    "print('질문 데이터의 크기(shape) :', questions.shape)\n",
    "print('답변 데이터의 크기(shape) :', answers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e81e917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8178, 8005, 7990, 2192,  919,   78,  821, 8179,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd5e8a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8178,   69, 2064,  456,    5,  137, 2188,   17,    1, 8179,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826d0c0b",
   "metadata": {},
   "source": [
    "## Step 4. 모델 구성하기\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2160e6",
   "metadata": {},
   "source": [
    "Postional Encoding 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d17911d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포지셔널 인코딩 레이어\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "        \n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2*(i//2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "    \n",
    "    def positional_encoding(self, position, d_model):\n",
    "        # 각도 배열 생성\n",
    "        angle_rads = self.get_angles(position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "                                    i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "                                    d_model=d_model)\n",
    "        # 배열의 짝수 인덱스에는 sin 함수 적용\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        # sin과 cosine이 교차되도록 재배열\n",
    "        pos_encoding = tf.stack([sines, cosines], axis=0)\n",
    "        pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
    "        pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
    "\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f16c452",
   "metadata": {},
   "source": [
    "스케일드 닷 프로덕트 어텐션 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35bc2f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케일드 닷 프로덕트 어텐션 함수\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # 가중치를 정규화\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # 패딩에 마스크 추가\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "  # softmax적용\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4e2344",
   "metadata": {},
   "source": [
    "멀티헤드어텐션 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd219a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "             'value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # Q, K, V에 각각 Dense를 적용합니다\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # 스케일드 닷 프로덕트 어텐션 함수\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "        # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0d0dea",
   "metadata": {},
   "source": [
    "교사 강요를 위해 디코더 입력은 END_TOKEN 제외하고 디코더 출력은 START_TOKEN 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63cd9a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(({'inputs': questions,\n",
    "                                              'dec_inputs': answers[:, :-1]},\n",
    "                                             {'outputs':answers[:, 1:]}))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95964df",
   "metadata": {},
   "source": [
    "패딩 마스킹 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "392318ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75e86ff",
   "metadata": {},
   "source": [
    "Lookahead mask 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e78a809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eeec90",
   "metadata": {},
   "source": [
    "하나의 인코더 층 구현(self attention과 feed forward 층으로 이루어짐)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55da3c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "    # 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "    attention = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention\")({\n",
    "            'query': inputs,\n",
    "            'key': inputs,\n",
    "            'value': inputs,\n",
    "            'mask': padding_mask})\n",
    "\n",
    "    # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "    # 두 번째 서브 레이어 : 2개의 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8aabe1",
   "metadata": {},
   "source": [
    "인코더 층을 임베딩 층(Embedding layer)과 포지셔널 인코딩(Positional Encoding)을 연결하고, 인코더 층을 원하는 만큼 쌓아 트랜스포머의 인코더 완성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34453f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "    # 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "  \n",
    "    # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    # num_layers만큼 쌓아올린 인코더의 층.\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(units=units,\n",
    "                                d_model=d_model,\n",
    "                                num_heads=num_heads,\n",
    "                                dropout=dropout,\n",
    "                                name=\"encoder_layer_{}\".format(i))([outputs, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc17e45",
   "metadata": {},
   "source": [
    "하나의 디코더 층 구현(3개의 서브 층으로 구성됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81171dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input( shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "    attention1 = MultiHeadAttention(d_model, \n",
    "                                    num_heads, \n",
    "                                    name=\"attention_1\")(inputs={'query': inputs,\n",
    "                                                                'key': inputs,\n",
    "                                                                'value': inputs,\n",
    "                                                                'mask': look_ahead_mask})\n",
    "\n",
    "    # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "    # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
    "    attention2 = MultiHeadAttention(d_model, num_heads, name=\"attention_2\")(inputs={'query': attention1,\n",
    "                                                                                    'key': enc_outputs,\n",
    "                                                                                    'value': enc_outputs,\n",
    "                                                                                    'mask': padding_mask})\n",
    "\n",
    "    # 마스크드 멀티 헤드 어텐션의 결과는\n",
    "    # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "    # 세 번째 서브 레이어 : 2개의 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "                          outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064d027e",
   "metadata": {},
   "source": [
    "인코더 층과 마찬가지로 임베딩 층(Embedding layer)과 포지셔널 인코딩(Positional Encoding)을 연결하고, 원하는 만큼 디코더 층을 쌓아 트랜스포머의 디코더 완성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "063572b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "    # 패딩 마스크\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "  \n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(units=units,\n",
    "                                d_model=d_model,\n",
    "                                num_heads=num_heads,\n",
    "                                dropout=dropout,\n",
    "                                name='decoder_layer_{}'.format(i))(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "                          outputs=outputs,name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c73345c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "583e3355",
   "metadata": {},
   "source": [
    "트랜스포머 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd42271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(vocab_size, num_layers, units, d_model, num_heads, dropout, name='transformer'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name='dec_inputs')\n",
    "    \n",
    "    # 인코더에서 패딩을 위한 마스크\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(create_padding_mask,\n",
    "                                             output_shape=(1, 1, None),\n",
    "                                             name='enc_padding_mask')(inputs)\n",
    "    # 디코더에서 미래의 토큰을 마스크 하기 위해 사용\n",
    "    # 내부적으로 패딩 마스크도 포함되어 있음\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(create_look_ahead_mask,\n",
    "                                            output_shape=(1, None, None),\n",
    "                                            name='look_ahead_mask')(dec_inputs)\n",
    "    # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
    "    # 디코더에서 패딩을 위한 마스크\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(create_padding_mask,\n",
    "                                             output_shape=(1, 1, None),\n",
    "                                             name='dec_padding_mask')(inputs)\n",
    "    \n",
    "    # Encoder\n",
    "    enc_outputs = encoder(vocab_size=vocab_size, \n",
    "                          num_layers=num_layers, \n",
    "                          units=units, \n",
    "                          d_model=d_model, \n",
    "                          num_heads=num_heads, \n",
    "                          dropout=dropout)(inputs=[inputs, enc_padding_mask])\n",
    "    \n",
    "    # Decoder\n",
    "    dec_outputs = decoder(vocab_size=vocab_size, \n",
    "                          num_layers=num_layers, \n",
    "                          units=units, \n",
    "                          d_model=d_model, \n",
    "                          num_heads=num_heads, \n",
    "                          dropout=dropout)(inputs=[dec_inputs, enc_outputs,\n",
    "                                                  look_ahead_mask,\n",
    "                                                  dec_padding_mask])\n",
    "    \n",
    "    # 완전 연결층\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name='outputs')(dec_outputs)\n",
    "    \n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ae11b30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 256)    3148288     inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 256)    3675648     dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8180)   2102260     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,926,196\n",
      "Trainable params: 8,926,196\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "D_MODEL = 256  # 인코더와 디코어 내부의 입, 출력 차원\n",
    "NUM_LAYERS = 2 # 인코더와 디코더 층의 개수\n",
    "NUM_HEADS= 8  # 멀티헤드 어텐션의 헤드 수\n",
    "UNITS = 512    # feed forward 신경망의 은닉층 크기\n",
    "DROPOUT = 0.1\n",
    "\n",
    "model = transformer(vocab_size = VOCAB_SIZE,\n",
    "                   num_layers = NUM_LAYERS,\n",
    "                   units = UNITS,\n",
    "                   d_model = D_MODEL,\n",
    "                   num_heads = NUM_HEADS,\n",
    "                   dropout = DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f7031d",
   "metadata": {},
   "source": [
    "손실 함수 정의(레이블인 시퀀스에 패딩이 되어있으므로 loss를 계산할 때 패딩 마스크를 적용해야 함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "204557ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  \n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6528503",
   "metadata": {},
   "source": [
    "커스텀 학습률(Learning rate) 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a1429a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    " \n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2c4831",
   "metadata": {},
   "source": [
    "모델 컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5753c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4b34a9",
   "metadata": {},
   "source": [
    "모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a8a0b517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "185/185 [==============================] - 27s 43ms/step - loss: 2.2660 - accuracy: 0.0457\n",
      "Epoch 2/50\n",
      "185/185 [==============================] - 8s 43ms/step - loss: 1.8354 - accuracy: 0.0773\n",
      "Epoch 3/50\n",
      "185/185 [==============================] - 8s 42ms/step - loss: 1.5675 - accuracy: 0.0792\n",
      "Epoch 4/50\n",
      "185/185 [==============================] - 8s 43ms/step - loss: 1.4492 - accuracy: 0.0849\n",
      "Epoch 5/50\n",
      "185/185 [==============================] - 8s 43ms/step - loss: 1.3572 - accuracy: 0.0901\n",
      "Epoch 6/50\n",
      "185/185 [==============================] - 8s 43ms/step - loss: 1.2649 - accuracy: 0.0966\n",
      "Epoch 7/50\n",
      "185/185 [==============================] - 8s 43ms/step - loss: 1.1641 - accuracy: 0.1055\n",
      "Epoch 8/50\n",
      "185/185 [==============================] - 8s 43ms/step - loss: 1.0518 - accuracy: 0.1170\n",
      "Epoch 9/50\n",
      "185/185 [==============================] - 8s 43ms/step - loss: 0.9277 - accuracy: 0.1307\n",
      "Epoch 10/50\n",
      "185/185 [==============================] - 8s 43ms/step - loss: 0.7987 - accuracy: 0.1458\n",
      "Epoch 11/50\n",
      "185/185 [==============================] - 8s 43ms/step - loss: 0.6683 - accuracy: 0.1623\n",
      "Epoch 12/50\n",
      "185/185 [==============================] - 8s 43ms/step - loss: 0.5405 - accuracy: 0.1799\n",
      "Epoch 13/50\n",
      "185/185 [==============================] - 8s 43ms/step - loss: 0.4241 - accuracy: 0.1962\n",
      "Epoch 14/50\n",
      "185/185 [==============================] - 8s 43ms/step - loss: 0.3220 - accuracy: 0.2122\n",
      "Epoch 15/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.2379 - accuracy: 0.2263\n",
      "Epoch 16/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.1726 - accuracy: 0.2385\n",
      "Epoch 17/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.1262 - accuracy: 0.2470\n",
      "Epoch 18/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.0976 - accuracy: 0.2520\n",
      "Epoch 19/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.0800 - accuracy: 0.2551\n",
      "Epoch 20/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.0715 - accuracy: 0.2565\n",
      "Epoch 21/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.0647 - accuracy: 0.2576\n",
      "Epoch 22/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.0634 - accuracy: 0.2576\n",
      "Epoch 23/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.0580 - accuracy: 0.2589\n",
      "Epoch 24/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.0495 - accuracy: 0.2608\n",
      "Epoch 25/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.0448 - accuracy: 0.2618\n",
      "Epoch 26/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.0384 - accuracy: 0.2635\n",
      "Epoch 27/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.0343 - accuracy: 0.2646\n",
      "Epoch 28/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.0319 - accuracy: 0.2651\n",
      "Epoch 29/50\n",
      "185/185 [==============================] - 8s 45ms/step - loss: 0.0285 - accuracy: 0.2661\n",
      "Epoch 30/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.0261 - accuracy: 0.2667\n",
      "Epoch 31/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.0236 - accuracy: 0.2671\n",
      "Epoch 32/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.0223 - accuracy: 0.2676\n",
      "Epoch 33/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.0204 - accuracy: 0.2682\n",
      "Epoch 34/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.0192 - accuracy: 0.2683\n",
      "Epoch 35/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.0185 - accuracy: 0.2685\n",
      "Epoch 36/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.0161 - accuracy: 0.2692\n",
      "Epoch 37/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.0161 - accuracy: 0.2693\n",
      "Epoch 38/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.0145 - accuracy: 0.2696\n",
      "Epoch 39/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.0150 - accuracy: 0.2695\n",
      "Epoch 40/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.0134 - accuracy: 0.2699\n",
      "Epoch 41/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.0124 - accuracy: 0.2702\n",
      "Epoch 42/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.0120 - accuracy: 0.2702\n",
      "Epoch 43/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.0116 - accuracy: 0.2704\n",
      "Epoch 44/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.0111 - accuracy: 0.2703\n",
      "Epoch 45/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.0112 - accuracy: 0.2704\n",
      "Epoch 46/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.0106 - accuracy: 0.2706\n",
      "Epoch 47/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.0096 - accuracy: 0.2708\n",
      "Epoch 48/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.0093 - accuracy: 0.2709\n",
      "Epoch 49/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.0096 - accuracy: 0.2708\n",
      "Epoch 50/50\n",
      "185/185 [==============================] - 8s 44ms/step - loss: 0.0086 - accuracy: 0.2710\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "history = model.fit(dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f99985d",
   "metadata": {},
   "source": [
    "## Step 5. 모델 평가하기\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5c098854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "38d984e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_inference(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
    "  # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
    "    sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
    "  # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
    "    output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  # 디코더의 인퍼런스 단계\n",
    "    for i in range(MAX_LENGTH):\n",
    "    # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
    "        predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "\n",
    "    # 현재 예측한 단어의 정수\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
    "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "              break\n",
    "\n",
    "    # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
    "    # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
    "        output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output_sequence, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ccc038cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(sentence):\n",
    "  # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
    "    prediction = decoder_inference(sentence)\n",
    "\n",
    "  # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "        [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "    print('입력 : {}'.format(sentence))\n",
    "    print('출력 : {}'.format(predicted_sentence))\n",
    "\n",
    "    return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6221e16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 너무 졸려\n",
      "출력 : 낮잠을 잠깐 자도 괜찮아요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'낮잠을 잠깐 자도 괜찮아요 .'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"너무 졸려\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1b4d7852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 저녁에 뭐 먹지?\n",
      "출력 : 한 번만 더 연락해보는 건 어떨까요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'한 번만 더 연락해보는 건 어떨까요 .'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"저녁에 뭐 먹지?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "35d6070b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 딥러닝은 참 어려워\n",
      "출력 : 구구릿빛 피부 좋죠 !\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'구구릿빛 피부 좋죠 !'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"딥러닝은 참 어려워\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "82b72878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 배고파\n",
      "출력 : 뭐 좀 챙겨드세요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'뭐 좀 챙겨드세요 .'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"배고파\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b7543d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 저녁 추천해줘\n",
      "출력 : 맛있는 거 드세요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'맛있는 거 드세요 .'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"저녁 추천해줘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "de929242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 무슨 노래 좋아해?\n",
      "출력 : 저도 부러워요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'저도 부러워요 .'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"무슨 노래 좋아해?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "45865f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 대학원 진학에 대해서 어떻게 생각해?\n",
      "출력 : 꿈에 도전하는 건 좋은 거라고 들었어요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'꿈에 도전하는 건 좋은 거라고 들었어요 .'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"대학원 진학에 대해서 어떻게 생각해?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9706f67e",
   "metadata": {},
   "source": [
    "## 회고 및 정리\n",
    "- 전체적인 과제 실행은 어려움이 없었으나, 트랜스포머 모델 구현에 사용되는 각 층 및 component에 대한 개념이 생소하고 복잡해서 성능 개선보다 이해하고 정리하는데에 초점을 두었다. \n",
    "\n",
    "\n",
    "1. 교사 강요(Teacher Forcing)\n",
    "    - 정의 : __테스트 과정__ 에서 t 시점의 출력이 t+1 시점의 입력으로 사용되는 훈련 기법. __훈련시__ 사용할 경우, 모델이 t 시점에서 예측한 값을 t+1 시점에 입력으로 사용하지 않고, t 시점의 레이블 즉, 실제 알고있는 정답을 t+1 시점의 입력으로 사용함.\n",
    "    - 교사 강요를 하지 않으면 잘못된 예측이 다음 시점의 입력으로 들어가기 때문에 연쇄적으로 예측 정확도에 영향을 미침.\n",
    "    - 즉, 훈련 과정에서 훈련 속도가 느려지고 성능 개선이 어려움.\n",
    "    - __트랜스포머 모델의 디코더에 교사 강요가 적용됨__\n",
    "    \n",
    "    \n",
    "2. tf.data.Dataset API (훈련 프로세스의 속도가 빨라지도록 입력 파이프라인을 구축하는 API)\n",
    "    - __Prefetching(가져오기)__\n",
    "    - 모델이 s 스텝 훈련을 실행하는 동안 입력 파이프라인은 s+1 스텝의 데이터를 읽어서 데이터가 학습되는 시간과 데이터가 생성되는 시간 사이의 간극을 최소화함.\n",
    " > dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    " \n",
    " \n",
    "3. Padding masking과 look-ahead masking\n",
    "    - __Padding masking__\n",
    "    - padding으로 채워진 무의미한 단어를 마스크로 가려서 실제 연산에 방해가 되지 않도록 하는 기법.\n",
    "    - 자연어 처리에서 padding은 문장 길이를 동일하게 해주는 과정해서 특정 길이보다 짧은 문장을 padding으로 채움 (pad_sequences)\n",
    "    - __look-ahead masking__\n",
    "    - RNN은 각 step마다 단어가 순서대로 입력되지만, 트랜스포머는 전체 문장이 문장 행렬로 한꺼번에 들어가기 때문에 어순과 상관없이 모든 단어를 참고해서 다음 단어를 예측할 수 있게됨. \n",
    "    -다음에 나올 단어를 참고하지 않고 이전 단어들로부터 다음 단어를 예측하는 훈련을 할 수 있도록 Query 단어 뒤에 나오는 key 단어에 대해 마스킹을 진행함\n",
    "    \n",
    "    \n",
    "4. 트랜스포머의 인코더 층 (embedding + positional-encoding + Self-attention + feed forward Neural Network)\n",
    "    - __padding mask 사용__\n",
    "    - __positional-encoding__\n",
    "    - embedding layer에 positional encoding 레이어를 더해서 사용.\n",
    "    - positional encoding은 문장 행렬이 입력으로 들어가는 트랜스포머 특성상 단어 위치에따른 정보를 주기 위함.\n",
    "    - __Self-attention__\n",
    "    - 인코더의 입력으로 들어간 문장 내 단어들 사이의 유사도를 구함.\n",
    "    \n",
    "    \n",
    "5. 트랜스포머의 디코더 층 (embedding + positional-encoding + self attention + encoder-decoder attention + feed forward Neural Network)\n",
    "    - __padding mask , look ahead mask 사용__\n",
    "    - __Multihead attention(self attention)__\n",
    "    - 이미 생성된 앞 단어들과의 유사도를 구함.\n",
    "    - __encoder-decoder attention__\n",
    "    - 인코더-디코더 어텐션은 서로 다른 단어 목록(인코더 내 단어와 디코더 내 단어) 사이에서 유사도를 구함.\n",
    "    - Query(시점 t에서 생성된 단어)는 __디코더의 벡터__ 인 반면, Key와 Value는 __인코더의 벡터__ 임.\n",
    "    - 위 과정에 의해 인코더가 디코더에 정보를 전달 할 수 있음.\n",
    "    - __주의__\n",
    "    - 트랜스포머 모델에 사용되는 모든 어텐션 레이어는 스케일드 닷 프로덕트 어텐션을 멀티 헤드 어텐션으로 병렬적으로 수행함.\n",
    "    \n",
    "6. 손실함수\n",
    "    - 문장 시퀀스에 패딩이 되어있는 경우, loss를 계산할 때도 패딩 마스크를 적용해야 함.\n",
    "\n",
    "7. 커스텀 학습률 스케줄링(Custom Learning rate Scheduling)\n",
    "    - 모델학습 초기에 learning rate를 급격히 높였다가, 이후 train step이 진행됨에 따라 서서히 낮추어 가면서 안정적으로 수렴하게 하는 기법\n",
    "    - tf.keras.optimizers.schedules.LearningRateSchedule 클래스를 상속받아 사용 가능함."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
