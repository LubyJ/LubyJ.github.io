{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "841ce56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim==3.8.0\n",
      "  Downloading gensim-3.8.0.tar.gz (23.4 MB)\n",
      "     |████████████████████████████████| 23.4 MB 6.1 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.0) (1.21.4)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.0) (1.7.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.0) (1.16.0)\n",
      "Requirement already satisfied: smart_open>=1.7.0 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.0) (5.2.1)\n",
      "Building wheels for collected packages: gensim\n",
      "  Building wheel for gensim (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gensim: filename=gensim-3.8.0-cp39-cp39-linux_x86_64.whl size=23983933 sha256=09792dfaadfea056b1a628c9da2d9718222fc9a548e7b5f3ad5d1d6c2fa6db36\n",
      "  Stored in directory: /aiffel/.cache/pip/wheels/fb/18/fa/2a0084905c1be0cead09a515d05015c1f939afb16fd7893ff4\n",
      "Successfully built gensim\n",
      "Installing collected packages: gensim\n",
      "  Attempting uninstall: gensim\n",
      "    Found existing installation: gensim 4.1.2\n",
      "    Uninstalling gensim-4.1.2:\n",
      "      Successfully uninstalled gensim-4.1.2\n",
      "Successfully installed gensim-3.8.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade gensim==3.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da9af8a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T08:04:04.034179Z",
     "start_time": "2023-06-19T08:04:03.294826Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "import nltk\n",
    "import gensim\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f297b99",
   "metadata": {},
   "source": [
    "지난 노드에서 __챗봇과 번역기는 같은 집안__ 이라고 했던 말을 기억하시나요?\n",
    "앞서 배운 Seq2seq번역기와 Transfomer번역기에 적용할 수도 있겠지만, 이번 노드에서 배운 번역기 성능 측정법을 챗봇에도 적용해 봅시다. 배운 지식을 다양하게 활용할 수 있는 것도 중요한 능력이겠죠. 이번 프로젝트를 통해서 챗봇과 번역기가 같은 집안인지 확인해 보세요!\n",
    "\n",
    "### Step 1. 데이터 다운로드\n",
    "---\n",
    "아래 파일 (ChatbotData.csv) 로드. csv 파일을 읽는 데에는 pandas 라이브러리가 적합. 읽어 온 데이터의 질문과 답변을 각각 questions, answers 변수에 나눠서 저장하기\n",
    "\n",
    "* [songys/Chatbot_data](https://github.com/songys/Chatbot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccdd2faf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T08:04:05.852645Z",
     "start_time": "2023-06-19T08:04:05.789926Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11808</th>\n",
       "      <td>화장 안했는데 썸남이 영통 걸었어. 어떡해?</td>\n",
       "      <td>화장실 불빛으로 좀 멀리 가리고 해보세요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11809</th>\n",
       "      <td>확실히 날 좋아하는 걸 아는 남자랑 친구가 될 수 있을까?</td>\n",
       "      <td>그 사람을 위해서는 그러면 안돼요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11810</th>\n",
       "      <td>확실히 좋아하는 데도 관심 있는거 티 안내려고 선톡 안하고 일부러 늦게 보내고 그러...</td>\n",
       "      <td>많이 있어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11811</th>\n",
       "      <td>홧김에 짝남한테 고백했다.</td>\n",
       "      <td>화끈하시네요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11812</th>\n",
       "      <td>회사 짝남 오빠 게임 초대 톡 옴.</td>\n",
       "      <td>설렜을텐데 아쉽겠어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11813 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       Q  \\\n",
       "0                                                 12시 땡!   \n",
       "1                                            1지망 학교 떨어졌어   \n",
       "2                                           3박4일 놀러가고 싶다   \n",
       "3                                        3박4일 정도 놀러가고 싶다   \n",
       "4                                                PPL 심하네   \n",
       "...                                                  ...   \n",
       "11808                           화장 안했는데 썸남이 영통 걸었어. 어떡해?   \n",
       "11809                   확실히 날 좋아하는 걸 아는 남자랑 친구가 될 수 있을까?   \n",
       "11810  확실히 좋아하는 데도 관심 있는거 티 안내려고 선톡 안하고 일부러 늦게 보내고 그러...   \n",
       "11811                                     홧김에 짝남한테 고백했다.   \n",
       "11812                                회사 짝남 오빠 게임 초대 톡 옴.   \n",
       "\n",
       "                             A  label  \n",
       "0                   하루가 또 가네요.      0  \n",
       "1                    위로해 드립니다.      0  \n",
       "2                  여행은 언제나 좋죠.      0  \n",
       "3                  여행은 언제나 좋죠.      0  \n",
       "4                   눈살이 찌푸려지죠.      0  \n",
       "...                        ...    ...  \n",
       "11808  화장실 불빛으로 좀 멀리 가리고 해보세요.      2  \n",
       "11809      그 사람을 위해서는 그러면 안돼요.      2  \n",
       "11810                  많이 있어요.      2  \n",
       "11811                  화끈하시네요.      2  \n",
       "11812             설렜을텐데 아쉽겠어요.      2  \n",
       "\n",
       "[11813 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./ChatbotData_.csv')\n",
    "data.head(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9f207c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T08:04:06.956270Z",
     "start_time": "2023-06-19T08:04:06.941415Z"
    }
   },
   "outputs": [],
   "source": [
    "questions = data['Q']\n",
    "answers = data['A']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174553ff",
   "metadata": {},
   "source": [
    "# Step 2. 데이터 정제\n",
    "아래 조건을 만족하는 preprocess_sentence() 함수를 구현하세요.\n",
    "\n",
    "1. 영문자의 경우, 모두 소문자로 변환합니다.\n",
    "2. 영문자와 한글, 숫자, 그리고 주요 특수문자를 제외하곤 정규식을 활용하여 모두 제거합니다.\n",
    "\n",
    "문장부호 양옆에 공백을 추가하는 등 이전과 다르게 생략된 기능들은 우리가 사용할 토크나이저가 지원하기 때문에 굳이 구현하지 않아도 괜찮습니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c47519f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T08:04:07.797343Z",
     "start_time": "2023-06-19T08:04:07.789376Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'[^a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣0-9?.!,]+', ' ', sentence)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083df7a0",
   "metadata": {},
   "source": [
    "# Step 3. 데이터 토큰화\n",
    "\n",
    "토큰화에는 KoNLPy의 mecab 클래스를 사용합니다.\n",
    "\n",
    "아래 조건을 만족하는 build_corpus() 함수를 구현하세요!\n",
    "\n",
    "1. 소스 문장 데이터와 타겟 문장 데이터를 입력으로 받습니다.\n",
    "2. 데이터를 앞서 정의한 preprocess_sentence() 함수로 정제하고, 토큰화합니다.\n",
    "3. 토큰화는 전달받은 토크나이즈 함수를 사용합니다. 이번엔 mecab.morphs 함수를 전달하시면 됩니다.\n",
    "4. 토큰의 개수가 일정 길이 이상인 문장은 데이터에서 제외합니다.\n",
    "5. 중복되는 문장은 데이터에서 제외합니다. 소스 : 타겟 쌍을 비교하지 않고 소스는 소스대로 타겟은 타겟대로 검사합니다. 중복 쌍이 흐트러지지 않도록 유의하세요!\n",
    "\n",
    "구현한 함수를 활용하여 questions 와 answers 를 각각 que_corpus , ans_corpus 에 토큰화하여 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "488a56bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T08:05:10.737633Z",
     "start_time": "2023-06-19T08:05:10.684902Z"
    }
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "from collections import defaultdict\n",
    "\n",
    "m = Mecab()\n",
    "\n",
    "def build_corpus(src_data, tgt_data):\n",
    "    \"\"\" 전처리 \"\"\"\n",
    "    preprocessed_src = list(map(preprocess_sentence, src_data))\n",
    "    preprocessed_tgt = list(map(preprocess_sentence, tgt_data))\n",
    "    \n",
    "    \"\"\"토큰화\"\"\"\n",
    "    tok_sources = []\n",
    "    tok_targets = []\n",
    "    for src, tgt in zip(preprocessed_src, preprocessed_tgt):\n",
    "        tok_src = m.morphs(src)\n",
    "        tok_sources.append(tok_src)\n",
    "        \n",
    "        tok_tgt = m.morphs(tgt)\n",
    "        tok_targets.append(tok_tgt)\n",
    "    \n",
    "    \"\"\"토큰 개수 15 이상인 문장 삭제\"\"\"    \n",
    "    drop_index_list = []\n",
    "    for i, (src, tgt) in enumerate(zip(tok_sources, tok_targets)):\n",
    "        if len(src) > 15 or len(tgt)> 15:\n",
    "            drop_index_list.append(i)\n",
    "    \n",
    "    new_src_data = [tok_sources[i] for i in range(len(tok_sources)) if i not in drop_index_list]\n",
    "    new_tgt_data = [tok_targets[i] for i in range(len(tok_targets)) if i not in drop_index_list]\n",
    "    \n",
    "    \"\"\"중복 문장 제거\"\"\"\n",
    "    index_dict_src = defaultdict(list)\n",
    "    duplicates_src = []\n",
    "    index_dict_tgt = defaultdict(list)\n",
    "    duplicates_tgt = []\n",
    "\n",
    "    # Find duplicates and track their indexes\n",
    "    for i, (src, tgt) in enumerate(zip(new_src_data, new_tgt_data)):\n",
    "        index_dict_src[tuple(src)].append(i)\n",
    "        index_dict_tgt[tuple(tgt)].append(i)\n",
    "\n",
    "    # 2회 이상 중복된 데이터의 index들 추출 (unique값은 남기기 위해 2 번째 인덱스부터 추출)\n",
    "    for indices_src, indices_tgt in zip(index_dict_src.values(), index_dict_tgt.values()):\n",
    "        if len(indices_src) > 1 or len(indices_tgt) > 1:\n",
    "            duplicates_src.extend(indices_src[1:])\n",
    "            duplicates_tgt.extend(indices_tgt[1:])\n",
    "        \n",
    "    duplicates_all = list(set(duplicates_src+duplicates_tgt))\n",
    "\n",
    "    # 중복 데이터 제거(처음 등장한 문장은 keep)\n",
    "    unique_src = [x for i, x in enumerate(new_src_data) if i not in duplicates_all]\n",
    "    unique_tgt = [x for i, x in enumerate(new_tgt_data) if i not in duplicates_all]\n",
    "    \n",
    "    \n",
    "    return tok_sources, tok_targets, unique_src, unique_tgt\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59abad6e",
   "metadata": {},
   "source": [
    "__토큰화된 문장의 길이 분포 확인__<br>\n",
    "\n",
    "토큰화된 Q와 A의 길이 제한을 15로 둘 경우 각각 3~5% 정도 데이터가 유실됨. 매우 적은 수치이므로 길이 제한을 15로 설정함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc627e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_sources, tok_targets, que_corpus, ans_corpus = build_corpus(questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2d97ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 최소 길이 : 1\n",
      "질문 최대 길이 : 32\n",
      "질문 평균 길이 : 7.0266429840142095\n",
      "답변 최소 길이 : 1\n",
      "답변 최대 길이 : 40\n",
      "답변 평균 길이 : 8.376554174067495\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Q_len = [len(s) for s in tok_sources]\n",
    "A_len = [len(s) for s in tok_targets]\n",
    "\n",
    "print('질문 최소 길이 : {}'.format(np.min(Q_len)))\n",
    "print('질문 최대 길이 : {}'.format(np.max(Q_len)))\n",
    "print('질문 평균 길이 : {}'.format(np.mean(Q_len)))\n",
    "print('답변 최소 길이 : {}'.format(np.min(A_len)))\n",
    "print('답변 최대 길이 : {}'.format(np.max(A_len)))\n",
    "print('답변 평균 길이 : {}'.format(np.mean(A_len)))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(Q_len)\n",
    "plt.title('Question')\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(A_len)\n",
    "plt.title('Answer')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Question')\n",
    "plt.hist(Q_len, bins = 30)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "\n",
    "plt.title('Answer')\n",
    "plt.hist(A_len, bins = 30)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "920bdb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_len15 데이터 개수: 11489\n",
      "A_len15데이터 개수: 11337\n",
      "Q_길이15 이하 데이터 비율:  0.9717499788547745\n",
      "A_길이15 이하 데이터 비율:  0.958893681806648\n"
     ]
    }
   ],
   "source": [
    "Q_len15 = [s for s in Q_len if s <= 15]\n",
    "A_len15 = [s for s in A_len if s <= 15]\n",
    "\n",
    "print('Q_len15 데이터 개수:', len(Q_len15))\n",
    "print('A_len15데이터 개수:', len(A_len15))\n",
    "print('Q_길이15 이하 데이터 비율: ', len(Q_len15)/len(Q_len))\n",
    "print('A_길이15 이하 데이터 비율: ', len(A_len15)/len(A_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d442f064",
   "metadata": {},
   "source": [
    "__중복 데이터 지운 후 데이터 개수 및 Q-A pair 유지 확인__<br>\n",
    "\n",
    "질문 corpus와 답변 corpus의 데이터 개수가 __7097__ 개로 동일하고 __질문과 답변 쌍이 잘 유지__ 되어 있는 것을 확인함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bccf9a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique src list length: 7097\n",
      "Unique tgt list length: 7097\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique src list length:\", len(que_corpus))\n",
    "print(\"Unique tgt list length:\", len(ans_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9785d01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "que_corpus 7096 데이터 ['힘든', '연애', '좋', '은', '연애', '라는', '게', '무슨', '차이', '일까', '?']\n",
      "ans_corpus 7096 데이터 ['잘', '헤어질', '수', '있', '는', '사이', '여부', '인', '거', '같', '아요', '.']\n"
     ]
    }
   ],
   "source": [
    "print('que_corpus 7096 데이터',que_corpus[7096])\n",
    "print('ans_corpus 7096 데이터',ans_corpus[7096])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445913ab",
   "metadata": {},
   "source": [
    "# Step 4. Augmentation\n",
    "\n",
    "* 중복 데이터 처리 이후 데이터는 7100개가량으로 적은 편에 속함.\n",
    "* __Lexical Substitution__ 을 적용해 데이터 증강.\n",
    "\n",
    "* 아래 링크를 참고하여 __한국어로 사전 훈련된 Embedding 모델을 다운로드__\n",
    "* Korean (w) 가 Word2Vec으로 학습한 모델이며 용량도 적당하므로 사이트에서 Korean (w)를 찾아 다운로드하여, ko.bin 파일을 얻기\n",
    "\n",
    "* [Kyubyong/wordvectors](https://github.com/Kyubyong/wordvectors)\n",
    "\n",
    "* 다운로드한 모델을 활용해 __데이터를 Augmentation 하기!__\n",
    "* 앞서 정의한 lexical_sub() 함수를 참고\n",
    "\n",
    "* Augmentation된 que_corpus 와 원본 ans_corpus 가 병렬을 이루도록, 이후엔 반대로 원본 que_corpus 와 Augmentation된 ans_corpus 가 병렬을 이루도록 하여 __전체 데이터가 원래의 3배가량으로 늘어나도록__ 하기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9564317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "wv = Word2Vec.load('./ko.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31a2f02",
   "metadata": {},
   "source": [
    "잘 동작하는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74da0b65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47/3095228173.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  wv.most_similar('감자')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('야채', 0.8413219451904297),\n",
       " ('돼지고기', 0.8353288769721985),\n",
       " ('닭고기', 0.828442394733429),\n",
       " ('육류', 0.8231872320175171),\n",
       " ('옥수수', 0.8214879631996155),\n",
       " ('양배추', 0.8204459547996521),\n",
       " ('토마토', 0.817166805267334),\n",
       " ('밀가루', 0.8060519695281982),\n",
       " ('올리브유', 0.805986225605011),\n",
       " ('고구마', 0.8036344051361084)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('감자')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29df6f10",
   "metadata": {},
   "source": [
    "한 문장에서 한 단어(토큰)만 유사어로 바꾸는 lexical_sub() 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e5815a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_sub(sentence, wv):\n",
    "    selected_tok = random.choice(sentence)\n",
    "\n",
    "    result = []\n",
    "    for tok in sentence:\n",
    "        if tok == selected_tok:\n",
    "            try:\n",
    "                similar_words = wv.most_similar(tok)\n",
    "                if len(similar_words) > 0:\n",
    "                    result += similar_words[0][0] + ' '\n",
    "            except KeyError:\n",
    "                result.append(tok)\n",
    "        else:\n",
    "            result.append(tok)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f419f22a",
   "metadata": {},
   "source": [
    "기존 데이터 쌍, 증강질문-기존답변 쌍, 기존질문-증강답변 쌍을 반환하는 augment_dataset() 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a90dd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_dataset(questions, answers, wv):\n",
    "    augmented_questions = []\n",
    "    augmented_answers = []\n",
    "    \n",
    "    for question, answer in zip(questions, answers):\n",
    "        augmented_questions.append(question)\n",
    "        augmented_answers.append(answer)\n",
    "        \n",
    "        augmented_question = lexical_sub(question, wv)\n",
    "        augmented_answer = lexical_sub(answer, wv)\n",
    "        \n",
    "        augmented_questions.append(augmented_question)\n",
    "        augmented_answers.append(answer)\n",
    "        \n",
    "        augmented_questions.append(question)\n",
    "        augmented_answers.append(augmented_answer)\n",
    "    \n",
    "    return augmented_questions, augmented_answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896e89e8",
   "metadata": {},
   "source": [
    "데이터 증강이 잘 구현되는지 확인<br>\n",
    "* 하나의 문장에서 하나의 단어만 유사어로 바뀜\n",
    "* 전체 데이터가 원래 대비 3배 늘어남"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f217b6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: ['12', '시', '땡', '!']\n",
      "A: ['하루', '가', '또', '가', '네요', '.']\n",
      "\n",
      "Q: ['12', '시', '끗', ' ', '!']\n",
      "A: ['하루', '가', '또', '가', '네요', '.']\n",
      "\n",
      "Q: ['12', '시', '땡', '!']\n",
      "A: ['일', '주', '일', ' ', '가', '또', '가', '네요', '.']\n",
      "\n",
      "Q: ['가스', '비', '너무', '많이', '나왔', '다', '.']\n",
      "A: ['다음', '달', '에', '는', '더', '절약', '해', '봐요', '.']\n",
      "\n",
      "Q: ['가스', '비', '너무', '많이', '나왔', '으', '며', ' ', '.']\n",
      "A: ['다음', '달', '에', '는', '더', '절약', '해', '봐요', '.']\n",
      "\n",
      "Q: ['가스', '비', '너무', '많이', '나왔', '다', '.']\n",
      "A: ['다음', '달', '에', 'ㄴ', '다', '는', ' ', '더', '절약', '해', '봐요', '.']\n",
      "\n",
      "Q: ['취향', '좀', '존중', '해', '줬', '으면']\n",
      "A: ['각자', '마다', '취향', '이', '있', '는', '거', '죠', '.']\n",
      "\n",
      "Q: ['취향', '조', '금', ' ', '존중', '해', '줬', '으면']\n",
      "A: ['각자', '마다', '취향', '이', '있', '는', '거', '죠', '.']\n",
      "\n",
      "Q: ['취향', '좀', '존중', '해', '줬', '으면']\n",
      "A: ['각자', '마다', '취향', '그', '러', ' ', '있', '는', '거', '죠', '.']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47/3742043412.py:8: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  similar_words = wv.most_similar(tok)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "questions = [['12', '시', '땡', '!'], \n",
    "             ['가스', '비', '너무', '많이', '나왔', '다', '.'], \n",
    "             ['취향', '좀', '존중', '해', '줬', '으면']]\n",
    "answers = [['하루', '가', '또', '가', '네요', '.'], \n",
    "           ['다음', '달', '에', '는', '더', '절약', '해', '봐요', '.'], \n",
    "           ['각자', '마다', '취향', '이', '있', '는', '거', '죠', '.']]\n",
    "\n",
    "augmented_questions, augmented_answers = augment_dataset(questions, answers, wv)\n",
    "\n",
    "# Print augmented question-answer pairs\n",
    "for i in range(len(augmented_questions)):\n",
    "    print(\"Q:\", augmented_questions[i])\n",
    "    print(\"A:\", augmented_answers[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25e30c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47/3742043412.py:8: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  similar_words = wv.most_similar(tok)\n"
     ]
    }
   ],
   "source": [
    "aug_que_corpus, aug_ans_corpus = augment_dataset(que_corpus, ans_corpus, wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdb7e504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 증강 뒤 Question 데이터 개수:  21291\n",
      "데이터 증강 뒤 Answer 데이터 개수:  21291\n"
     ]
    }
   ],
   "source": [
    "print('데이터 증강 뒤 Question 데이터 개수: ', len(aug_que_corpus))\n",
    "print('데이터 증강 뒤 Answer 데이터 개수: ', len(aug_ans_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0f03bb",
   "metadata": {},
   "source": [
    "# Step 5. 데이터 벡터화\n",
    "\n",
    "\n",
    "\n",
    "__1. 타겟 데이터(answer) 전체에 \\<start> 토큰과 \\<end> 토큰 추가__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4819298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_start_end_tokens(sentences):\n",
    "    augmented_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        augmented_sentence = [\"<start>\"] + sentence + [\"<end>\"]\n",
    "        augmented_sentences.append(augmented_sentence)\n",
    "    \n",
    "    return augmented_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9719bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_ans_corpus2 = add_start_end_tokens(aug_ans_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443b68a9",
   "metadata": {},
   "source": [
    "__2. 전체 데이터(aug_que_corpus, aug_ans_corpus2)에 대한 단어 사전 구축 및 벡터화__ \n",
    "\n",
    "enc_train 과 dec_train 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c2d2340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def vectorization(questions, answers, num_words=20000):\n",
    "    counter = Counter()\n",
    "\n",
    "    for q, a in zip(questions, answers):\n",
    "        counter.update(q)\n",
    "        counter.update(a)\n",
    "        \n",
    "    counter = counter.most_common(num_words-2)\n",
    "    vocab = ['<PAD>', '<UNK>'] + [key for key, _ in counter]\n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "    \n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index.get(word, word_to_index['<UNK>']) for word in wordlist]\n",
    "    \n",
    "    enc_train = []\n",
    "    dec_train = []\n",
    "    for wordlist_que, wordlist_ans in zip(questions, answers):\n",
    "        enc = wordlist_to_indexlist(wordlist_que)\n",
    "        dec = wordlist_to_indexlist(wordlist_ans)\n",
    "        enc_train.append(enc)\n",
    "        dec_train.append(dec)\n",
    "    \n",
    "    # counter, vocab은 확인용으로 반환함. 따로 쓰지는 않음\n",
    "    return enc_train, dec_train, word_to_index, counter, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ec3084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train, dec_train, word_to_index, counter, vocab = vectorization(aug_que_corpus, aug_ans_corpus2, num_words=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b8a59af",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_len = [len(s) for s in enc_train]\n",
    "A_len = [len(s) for s in dec_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "edb79826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 최대 길이 : 26\n",
      "답변 최대 길이 : 23\n"
     ]
    }
   ],
   "source": [
    "print('질문 최대 길이 : {}'.format(np.max(Q_len)))\n",
    "print('답변 최대 길이 : {}'.format(np.max(A_len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e53eb36",
   "metadata": {},
   "source": [
    "데이터 증강을 하면서 최대 길이가 15에서 각각 21, 24로 증가함. MAX_LEN 30 에 맞춰 Padding 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4944cbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 30\n",
    "\n",
    "enc_train_pad = []\n",
    "dec_train_pad = []\n",
    "\n",
    "for enc, dec in zip(enc_train, dec_train):\n",
    "    enc_pad = enc + [0] * (MAX_LEN - len(enc))\n",
    "    dec_pad = dec + [0] * (MAX_LEN - len(dec))\n",
    "    enc_train_pad.append(enc_pad)\n",
    "    dec_train_pad.append(dec_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2840a4e4",
   "metadata": {},
   "source": [
    "질문과 답변 데이터를 묶어 배치 크기의 텐서로 만들어 주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b53012d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((enc_train_pad, dec_train_pad)).batch(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131c4742",
   "metadata": {},
   "source": [
    "# Step 6. 훈련하기\n",
    "\n",
    "* 데이터 크기가 작아 과적합 되기 쉬우니 하이퍼파라미터 튜닝 필요\n",
    "> Hyperparameters<br>\n",
    "n_layers: 1<br>d_model: 368<br>n_heads: 8<br>d_ff: 1024<br>dropout: 0.2<br>Training Parameters<br>\n",
    "Warmup Steps: 1000<br>Batch Size: 64<br>Epoch At: 10<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c3ffe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding 구현\n",
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, (2*(i//2)) / np.float32(d_model))\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34a39467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask  생성하기\n",
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "\n",
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x) # 패딩마스크도 포함\n",
    "\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)\n",
    "\n",
    "# def generate_lookahead_mask(size):\n",
    "#     \"\"\"\n",
    "#     tf.linalg.band_part : 삼각 혹은 대각 행렬 구현 함수\n",
    "#     tf.linalg.band_part(input, num_lower, num_upper, name=None)\n",
    "#     num_lower 가 음수인 경우 하삼각행렬이 1로 채워짐\n",
    "#     num_upper 가 음수인 경우 상삼각행렬이 1로 채워짐\n",
    "#     대각 행렬이 0 으로 된 상삼각 행렬을 만들기 위해 '1 - ' 가 필요함.\n",
    "#     하삼각 행렬 생성 -> 대각 행렬이 0인 상삼각 행렬 생성\n",
    "#     \"\"\"\n",
    "#     mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0) # 하삼각행렬이 1로 채워진 행렬 생성\n",
    "#     return mask\n",
    "\n",
    "def generate_lookahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = generate_padding_mask(x) # 패딩마스크도 포함\n",
    "\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)\n",
    "\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    dec_mask = generate_lookahead_mask(tgt)\n",
    "#    dec_tgt_padding_mask = generate_padding_mask(tgt)\n",
    "#    dec_mask = tf.maximum(dec_tgt_padding_mask, dec_lookahead_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb7b32a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Head Attention 구현\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    # 밑으로 코드 구현\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "        \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "                        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "            \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "858a758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position-wise Feed Forward Network 구현\n",
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72382830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder의 레이어 구현\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d80c96d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder 레이어 구현\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        '''\n",
    "        Masked Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        # Q, K, V 순서에 주의하세요!\n",
    "        out, dec_enc_attn = self.enc_dec_attn(Q=out, K=enc_out, V=enc_out, mask=dec_enc_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fdfe801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder 구현\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "    \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        # TODO: 구현\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8677793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder 구현\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "                            \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        # TODO: 구현\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, dec_enc_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "            \n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7e3f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        # TODO: 구현\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "        return out\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, dec_enc_mask, dec_mask):\n",
    "        # TODO: 구현\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, dec_enc_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91e15cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 하이퍼파라미터로 Transformer 인스턴스 생성\n",
    "transformer = Transformer(\n",
    "    n_layers=1,\n",
    "    d_model=368,\n",
    "    n_heads=8,\n",
    "    d_ff=1024,\n",
    "    dropout=0.2,\n",
    "    src_vocab_size = len(vocab) + 2,\n",
    "    tgt_vocab_size = len(vocab) + 2,\n",
    "    pos_len=30,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\n",
    "d_model = 368"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "92e0d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Scheduler 구현\n",
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=1000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        # TODO: 구현\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "774c9ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate 인스턴스 선언 & Optimizer 구현\n",
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "281efc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function 정의\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    # TODO: 구현\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "14c66277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Step 정의\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    # TODO: 구현\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))    \n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "15a74d0e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce8bc595c4524a30b05307c16818d9e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 3.7512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a5a3d0d3bab4f76b4294f89cbc7d9db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.1028\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "901c463cd8e34fd7ad00c761cf89ddd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.7345\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed97d6a8103f4f89b68fbbb5b238e9f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 2.2706\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4ecf5a51154954b279ef1c9bd4d830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 1.6844\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aace79a380ba47d7bef7273cde54a15e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss: 1.2152\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f3f1f288294fe299bd048b290231de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss: 0.9364\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b92e0864f97474faa59240d92b78838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss: 0.7407\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f8191b63f842d1a7c6497fdf68bb38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss: 0.6044\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be725e79586a4bb5b8f4f87fd3bd3ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss: 0.5074\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    # 전체 데이터셋을 batchsize로 나눠서 전체 학습 batch 개수를 반환함 즉, 1850\n",
    "    # round(118370 / 64) = 1850\n",
    "    dataset_count = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "    tqdm_bar = tqdm(total=dataset_count)\n",
    "    # [[YOUR CODE]]\n",
    "    for (batch, (src, tgt)) in enumerate(train_dataset):\n",
    "        \"\"\"\n",
    "        tf.data.experimental.cardinality(train_dataset).numpy() 이 코드로 이미 batch_size 만큼\n",
    "        데이터들을 slicing 해서 src, tgt를 슬라이싱 했으므로 따로 slicing 할 필요 없음.\n",
    "        \"\"\"\n",
    "        src = tf.cast(src, dtype=tf.int64)\n",
    "        tgt = tf.cast(tgt, dtype=tf.int64)\n",
    "        \n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = train_step(src, tgt, transformer, optimizer)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        # Display progress\n",
    "        tqdm_bar.set_description(f'Epoch {epoch+1}')\n",
    "        tqdm_bar.set_postfix(loss=total_loss.numpy()/(batch+1))\n",
    "        tqdm_bar.update(1)\n",
    "    \n",
    "    tqdm_bar.close()\n",
    "    print(f'Epoch {epoch+1} Loss: {total_loss.numpy()/(batch+1):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c1d8aa",
   "metadata": {},
   "source": [
    "# Step 7. 성능 측정하기\n",
    "\n",
    "1. 주어진 질문에 적절한 답변을 하는지 확인\n",
    "2. BLEU Score를 계산하는 calculate_bleu() 함수 이용해 수치적으로 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa21cf3",
   "metadata": {},
   "source": [
    "__1. 주어진 질문에 적절한 답변 하는지 확인__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d88e76a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fbebe3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index['<start>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8ec858e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(question, transformer):\n",
    "    preprocessed_src = preprocess_sentence(question)\n",
    "    que_temp = m.morphs(preprocessed_src)\n",
    "        \n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index.get(word, word_to_index['<UNK>']) for word in wordlist]\n",
    "    \n",
    "    enc_test = wordlist_to_indexlist(que_temp)\n",
    "    _input = enc_test + [0] * (MAX_LEN - len(enc_test))\n",
    "    _input = np.array(_input)\n",
    "    _input = np.expand_dims(_input, 0)\n",
    "\n",
    "    output = tf.expand_dims(word_to_index['<start>'], 0)\n",
    "\n",
    "    output = tf.cast(output, tf.int32)\n",
    "    \n",
    "    output_array = tf.TensorArray(dtype=tf.int32,\n",
    "                                      size=0,\n",
    "                                      dynamic_size=True)   \n",
    "    output_array = output_array.write(0, output)\n",
    "\n",
    "\n",
    "    for i in range(MAX_LEN):\n",
    "        output = tf.transpose(output_array.stack())\n",
    "        enc_mask, dec_enc_mask, dec_mask = generate_masks(_input, output)\n",
    "        \n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = transformer(_input, output, enc_mask, dec_enc_mask, dec_mask)\n",
    "        \n",
    "        predictions = predictions[:, -1:, :] # 출력 맨 마지막 단어 벡터\n",
    "        \n",
    "        predicted_id = tf.argmax(predictions, axis=-1) # Softmax로 가장 높은 확률의 단어 추출\n",
    "        predicted_id = tf.cast(predicted_id, dtype=tf.int32) # int64 로 변환\n",
    "        output_array = output_array.write(i + 1, predicted_id[0]) # output_array의 마지막 부분에 작성\n",
    "                  \n",
    "        if predicted_id == word_to_index['<end>']:\n",
    "            break\n",
    "          \n",
    "    output = tf.transpose(output_array.stack()).numpy()\n",
    "\n",
    "    \n",
    "    # Tok => \n",
    "    result=[]\n",
    "    for tok in output[0]:\n",
    "        word = index_to_word[tok]\n",
    "        result.append(word)\n",
    "        \n",
    "    return question, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dd8a61a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, transformer, plot_attention=False):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evalute(sentence, transformer)\n",
    "    \n",
    "    print('Question: {}'.format(sentence))\n",
    "    print('Answer: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3c8c434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "aef34d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\"홧김에 짝남한테 고백했다.\",\n",
    "            \"지루하다, 놀러가고 싶어.\",\n",
    "            \"오늘 일찍 일어났더니 피곤하다.\",\n",
    "            \"간만에 여자친구랑 데이트 하기로 했어.\",\n",
    "            \"집에 있는다는 소리야.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ef808821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>홧김에 짝남한테 고백했다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>지루하다, 놀러가고 싶어.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>오늘 일찍 일어났더니 피곤하다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>간만에 여자친구랑 데이트 하기로 했어.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>집에 있는다는 소리야.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  sample\n",
       "0         홧김에 짝남한테 고백했다.\n",
       "1         지루하다, 놀러가고 싶어.\n",
       "2      오늘 일찍 일어났더니 피곤하다.\n",
       "3  간만에 여자친구랑 데이트 하기로 했어.\n",
       "4           집에 있는다는 소리야."
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"sample\"] = examples\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "63275cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예문\n",
      "==================================================\n",
      "> 1. 홧김에 짝남한테 고백했다.\n",
      "> 2. 지루하다, 놀러가고 싶어.\n",
      "> 3. 오늘 일찍 일어났더니 피곤하다.\n",
      "> 4. 간만에 여자친구랑 데이트 하기로 했어.\n",
      "> 5. 집에 있는다는 소리야.\n",
      "\n",
      "\n",
      "답변\n",
      "==================================================\n",
      "> 1. 화끈 하 시 네요 .\n",
      "> 2. 생각 을 하 고 나 서 하 고 가 는 것 이 좋 겠 네요 .\n",
      "> 3. 사 는 것   처럼 , 나쁜 생각 이 나 봐요 .\n",
      "> 4. 좋 은 마무리 가 되 길 바랄게요 .\n",
      "> 5. 좋 은 사람 이 라면 먹 든 후폭풍 이 에요 .\n",
      "\n",
      "\n",
      "Hyperparameters\n",
      "==================================================\n",
      "n_layers : 1\n",
      "d_model : 368\n",
      "n_heads : 8\n",
      "d_ff : 1024\n",
      "dropout : 0.2\n",
      "Warmup_steps : 1000\n",
      "BATCH_SIZE  : 64\n",
      "EPOCHS  : 10\n"
     ]
    }
   ],
   "source": [
    "trans_list=[]\n",
    "print('예문')\n",
    "print('='*50)\n",
    "for idx, sen in enumerate(examples):\n",
    "    print(f'> {str(idx+1)}. {sen}')\n",
    "\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sen, transformer)\n",
    "    trans_list.append(result)\n",
    "print('\\n')\n",
    "print('답변')\n",
    "print('='*50)\n",
    "for idx, sen in enumerate(trans_list):\n",
    "    sen = sen[1:-1]\n",
    "    sen = ' '.join(sen)\n",
    "    print(f'> {str(idx+1)}. {sen}')\n",
    "\n",
    "print('\\n')\n",
    "print('Hyperparameters')\n",
    "print('='*50)\n",
    "\n",
    "n_layers =  1\n",
    "d_model = 368\n",
    "n_heads = 8\n",
    "d_ff = 1024\n",
    "dropout = 0.2\n",
    "warmup_steps = 1000\n",
    "\n",
    "print('n_layers :',n_layers)\n",
    "print('d_model :',d_model)\n",
    "print('n_heads :',n_heads)\n",
    "print('d_ff :',d_ff)\n",
    "print('dropout :',dropout)\n",
    "print('Warmup_steps :',warmup_steps)\n",
    "print('BATCH_SIZE  :',BATCH_SIZE )\n",
    "print('EPOCHS  :',EPOCHS )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c97ef29",
   "metadata": {},
   "source": [
    "__2. BLEU Score를 이용해 수치적으로 평가__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "02fe95ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.translate.bleu_score as bleu\n",
    "a = \"화끈하시네요.\".split()\n",
    "b = \"화끈 하 시 네요 .\".split()\n",
    "bleu.sentence_bleu( [b], a )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccc3ee8",
   "metadata": {},
   "source": [
    "# 회고\n",
    "\n",
    "* 이번 익스를 진행할 때, 토큰화를 하는 과정에서 케라스나 다른 라이브러리에 내장된 함수를 사용하지 않고, enumerate를 돌려 vocab과 토큰을 형성했는데, 이게 모든 재앙의 시작이 됨.\n",
    "* 데이터 타입이 맞지 않아서 패딩도 케라스 내장함수로 구현 못해서 손수 해줬어야 했음.\n",
    "* 더 문제는, 추론 과정(evaluate)에서 코드 한 줄 한 줄 실행할 때마다 온갖 에러가 다 났음.\n",
    "* 민수님의 도움으로 추론과정을 무사히 실행할 수 있게 됨.\n",
    "* 대부분의 문제는 'output'의 shape이 1D라 lookahead 마스크를  생성할 수 없거나 혹은, 어떤 코드는 'int32'를 default로 지정해서 반환하고 어떤 코드는 'int64'를 반환해서 충돌이 발생해 코드가 실행되지 않았음.\n",
    "* 그래서 evaluate 코드를 보면 dtype을 'int32'로 변환해주는 부분이 매우 많음.\n",
    "* 또 다른 문제는, 학습할 때 어떨 땐 문제없이 loss가 줄어들고 어떨 때는 loss가 nan으로 뜸. 이는, 코드를 돌릴 때마다 형태소 분석 이후 vocab size가 달라져서 embedding_dim이 vocab size보다 적게 입력된 것이 문제가 됨.\n",
    "* src와 tgt vocab size를 len(vocab) + 2 로 지정한 이후로는 문제가 나지 않았음.\n",
    "* bleu score는 데이터에 있는 Q&A 중 하나를 골라 평가함.\n",
    "* 점수는 0점인데 그런게 당연함. 형태소 분석을 거쳤기 때문에 띄어쓰기가 매우 많이 되어 있어서 단어를 토대로 점수를 평가하는 bleu가 하나도 안맞는다고 평가할 수밖에 없음.\n",
    "* 저 부분을 손보자니 기말고사 준비할 시간이 너무 부족할 것 같아 여기에서 만족하기로 함.....\n",
    "* 이번 익스를 하며 느낀건, 내장 함수를 똑똑하게 활용하자....임.\n",
    "* 코딩 실력, 코딩 해석, 내장함수 활용 등 다방면에서 나의 한계가 여실히 드러났던 주간이었음."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
